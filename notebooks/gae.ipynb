{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, ChebConv\n",
    "\n",
    "from torch_geometric.datasets import Planetoid, MNISTSuperpixels\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch_geometric.utils as torch_util\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTSuperpixels(60000)\n",
      "Data(edge_attr=[1399, 1], edge_index=[2, 1399], pos=[75, 2], x=[75, 1], y=[1])\n",
      "tensor([[ 0,  0,  0,  ..., 74, 74, 74],\n",
      "        [ 3,  8, 10,  ..., 55, 63, 69]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Only integers, slices (`:`) and long or byte tensors are valid indices (got str).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-649c6318c69b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# print(dataset[:]['edge_index'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_scipy_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     85\u001b[0m         raise IndexError(\n\u001b[1;32m     86\u001b[0m             \u001b[0;34m'Only integers, slices (`:`) and long or byte tensors are valid '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             'indices (got {}).'.format(type(idx).__name__))\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Only integers, slices (`:`) and long or byte tensors are valid indices (got str)."
     ]
    }
   ],
   "source": [
    "dataset = 'Mnist'\n",
    "path = '../data/geometric/MNIST'\n",
    "dataset = MNISTSuperpixels(path, dataset, T.Distance())\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "print(dataset)\n",
    "print(dataset[0])\n",
    "print(dataset[0]['edge_index'])\n",
    "# print(dataset['edge_index'])\n",
    "# print(dataset[:]['edge_index'])\n",
    "\n",
    "print(torch_util.to_scipy_sparse_matrix(dataset[:]['edge_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(nn.Module):\n",
    "    \"\"\"Encoder using GCN layers\"\"\"\n",
    "\n",
    "    def __init__(self, n_feat, n_hid, n_latent, dropout):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        \n",
    "#         self.gc1 = GraphConvolution(n_feat, n_hid)\n",
    "#         self.gc2_mu = GraphConvolution(n_hid, n_latent)\n",
    "#         self.gc2_sig = GraphConvolution(n_hid, n_latent)\n",
    "#         self.dropout = dropout\n",
    "        self.gc1 = GCNConv(n_feat, n_hid)\n",
    "        self.gc2_mu = GCNConv(n_hid, n_latent)\n",
    "        self.gc2_sig = GCNConv(n_hid, n_latent)\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # First layer shared between mu/sig layers\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        mu = self.gc2_mu(x, adj)\n",
    "        log_sig = self.gc2_sig(x, adj)\n",
    "        return mu, torch.exp(log_sig)\n",
    "\n",
    "\n",
    "class InnerProductDecoder(nn.Module):\n",
    "    \"\"\"Decoder for using inner product for prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, dropout):\n",
    "        super(InnerProductDecoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fudge = 1e-7\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.dropout(z, self.dropout, training=self.training)\n",
    "        adj = (self.sigmoid(torch.mm(z, z.t())) + self.fudge) * (1 - 2 * self.fudge)\n",
    "        return adj\n",
    "\n",
    "\n",
    "class GAE(nn.Module):\n",
    "    \"\"\"Graph Auto Encoder (see: https://arxiv.org/abs/1611.07308)\"\"\"\n",
    "\n",
    "    def __init__(self, data, n_hidden, n_latent, dropout, subsampling=False):\n",
    "        super(GAE, self).__init__()\n",
    "\n",
    "        # Data\n",
    "        self.x = data['features']\n",
    "        self.adj_norm = data['adj_norm']\n",
    "        self.adj_labels = data['adj_labels']\n",
    "        self.obs = self.adj_labels.view(1, -1)\n",
    "\n",
    "        # Dimensions\n",
    "        N, D = data['features'].shape\n",
    "        self.n_samples = N\n",
    "        self.n_edges = self.adj_labels.sum()\n",
    "        self.n_subsample = 2 * self.n_edges\n",
    "        self.input_dim = D\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_latent = n_latent\n",
    "\n",
    "        # Parameters\n",
    "        self.pos_weight = float(N * N - self.n_edges) / self.n_edges\n",
    "        self.norm = float(N * N) / ((N * N - self.n_edges) * 2)\n",
    "        self.subsampling = subsampling\n",
    "\n",
    "        # Layers\n",
    "        self.dropout = dropout\n",
    "        self.encoder = GCNEncoder(self.input_dim, self.n_hidden, self.n_latent, self.dropout)\n",
    "        self.decoder = InnerProductDecoder(self.dropout)\n",
    "\n",
    "\n",
    "    def model(self):\n",
    "        # register PyTorch module `decoder` with Pyro\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "\n",
    "        # Setup hyperparameters for prior p(z)\n",
    "        z_mu    = torch.zeros([self.n_samples, self.n_latent])\n",
    "        z_sigma = torch.ones([self.n_samples, self.n_latent])\n",
    "\n",
    "        # sample from prior\n",
    "        z = pyro.sample(\"latent\", dist.Normal(z_mu, z_sigma).to_event(2))\n",
    "\n",
    "        # decode the latent code z\n",
    "        z_adj = self.decoder(z).view(1, -1)\n",
    "\n",
    "        # Score against data\n",
    "        pyro.sample('obs', WeightedBernoulli(z_adj, weight=self.pos_weight).to_event(2), obs=self.obs)\n",
    "\n",
    "\n",
    "    def guide(self):\n",
    "        # register PyTorch model 'encoder' w/ pyro\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "\n",
    "        # Use the encoder to get the parameters use to define q(z|x)\n",
    "        z_mu, z_sigma = self.encoder(self.x, self.adj_norm)\n",
    "\n",
    "        # Sample the latent code z\n",
    "        pyro.sample(\"latent\", dist.Normal(z_mu, z_sigma).to_event(2))\n",
    "\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        z_mu, _ = self.encoder.eval()(self.x, self.adj_norm)\n",
    "        # Put encoder back into training mode\n",
    "        self.encoder.train()\n",
    "        return z_mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \"\"\" Train GAE \"\"\"\n",
    "    print(\"Using {} dataset\".format(args.dataset_str))\n",
    "    # Load data\n",
    "    np.random.seed(1)\n",
    "    adj, features = load_data(args.dataset_str)\n",
    "    N, D = features.shape\n",
    "\n",
    "    # Store original adjacency matrix (without diagonal entries)\n",
    "    adj_orig = adj\n",
    "    adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "\n",
    "    # Some preprocessing\n",
    "    adj_train_norm   = preprocess_graph(adj_train)\n",
    "    adj_train_norm   = Variable(make_sparse(adj_train_norm))\n",
    "    adj_train_labels = Variable(torch.FloatTensor(adj_train + sp.eye(adj_train.shape[0]).todense()))\n",
    "    features         = Variable(make_sparse(features))\n",
    "\n",
    "    n_edges = adj_train_labels.sum()\n",
    "\n",
    "    data = {\n",
    "        'adj_norm'  : adj_train_norm,\n",
    "        'adj_labels': adj_train_labels,\n",
    "        'features'  : features,\n",
    "    }\n",
    "\n",
    "    gae = GAE(data,\n",
    "              n_hidden=32,\n",
    "              n_latent=16,\n",
    "              dropout=args.dropout)\n",
    "\n",
    "    optimizer = Adam({\"lr\": args.lr, \"betas\": (0.95, 0.999)})\n",
    "\n",
    "    svi = SVI(gae.model, gae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "    # Results\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    # Full batch training loop\n",
    "    for epoch in range(args.num_epochs):\n",
    "        # initialize loss accumulator\n",
    "        epoch_loss = 0.\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step()\n",
    "\n",
    "        # report training diagnostics\n",
    "        normalized_loss = epoch_loss / (2 * N * N)\n",
    "\n",
    "        results['train_elbo'].append(normalized_loss)\n",
    "\n",
    "        # Training loss\n",
    "        emb = gae.get_embeddings()\n",
    "\n",
    "        accuracy, roc_curr, ap_curr = eval_gae(val_edges, val_edges_false, emb, adj_orig)\n",
    "\n",
    "        results['accuracy_train'].append(accuracy)\n",
    "        results['roc_train'].append(roc_curr)\n",
    "        results['ap_train'].append(ap_curr)\n",
    "\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "              \"train_loss=\", \"{:.5f}\".format(normalized_loss),\n",
    "              \"train_acc=\", \"{:.5f}\".format(accuracy), \"val_roc=\", \"{:.5f}\".format(roc_curr), \"val_ap=\", \"{:.5f}\".format(ap_curr))\n",
    "\n",
    "        # Test loss\n",
    "        if epoch % args.test_freq == 0:\n",
    "            emb = gae.get_embeddings()\n",
    "            accuracy, roc_score, ap_score = eval_gae(test_edges, test_edges_false, emb, adj_orig)\n",
    "            results['accuracy_test'].append(accuracy)\n",
    "            results['roc_test'].append(roc_curr)\n",
    "            results['ap_test'].append(ap_curr)\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test loss\n",
    "    emb = gae.get_embeddings()\n",
    "    accuracy, roc_score, ap_score = eval_gae(test_edges, test_edges_false, emb, adj_orig)\n",
    "    print('Test Accuracy: ' + str(accuracy))\n",
    "    print('Test ROC score: ' + str(roc_score))\n",
    "    print('Test AP score: ' + str(ap_score))\n",
    "\n",
    "    # Plot\n",
    "    plot_results(results, args.test_freq, path= args.dataset_str + \"_results.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Some functions borrowed from:\n",
    "# https://github.com/tkipf/pygcn and \n",
    "# https://github.com/tkipf/gae\n",
    "# ------------------------------------\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "def eval_gae(edges_pos, edges_neg, emb, adj_orig):\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    emb = emb.data.numpy()\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "\n",
    "    accuracy = accuracy_score((preds_all > 0.5).astype(float), labels_all)\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return accuracy, roc_score, ap_score\n",
    "\n",
    "\n",
    "def make_sparse(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row,\n",
    "                                          sparse_mx.col))).long()\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "# def load_data(dataset):\n",
    "#     # load the data: x, tx, allx, graph\n",
    "#     names = ['x', 'tx', 'allx', 'graph']\n",
    "#     objects = []\n",
    "#     for i in range(len(names)):\n",
    "#         objects.append(\n",
    "#             pkl.load(open(\"data/ind.{}.{}\".format(dataset, names[i]))))\n",
    "#     x, tx, allx, graph = tuple(objects)\n",
    "#     test_idx_reorder = parse_index_file(\n",
    "#         \"data/ind.{}.test.index\".format(dataset))\n",
    "#     test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "#     if dataset == 'citeseer':\n",
    "#         # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "#         # Find isolated nodes, add them as zero-vecs into the right position\n",
    "#         test_idx_range_full = range(\n",
    "#             min(test_idx_reorder), max(test_idx_reorder) + 1)\n",
    "#         tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "#         tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "#         tx = tx_extended\n",
    "\n",
    "#     features = sp.vstack((allx, tx)).tolil()\n",
    "#     features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "#     adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "#     return adj, features\n",
    "\n",
    "\n",
    "# Subsample sparse variables\n",
    "def get_subsampler(variable):\n",
    "    data = variable.view(1, -1).data.numpy()\n",
    "    edges = np.where(data == 1)[1]\n",
    "    nonedges = np.where(data == 0)[1]\n",
    "\n",
    "    def sampler():\n",
    "        idx = np.random.choice(\n",
    "            nonedges.shape[0], edges.shape[0], replace=False)\n",
    "        return np.append(edges, nonedges[idx])\n",
    "\n",
    "    return sampler\n",
    "\n",
    "\n",
    "def plot_results(results, test_freq, path='results.png'):\n",
    "    # Init\n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    x_axis_train = range(len(results['train_elbo']))\n",
    "    x_axis_test = range(0, len(x_axis_train), test_freq)\n",
    "    # Elbo\n",
    "    ax = fig.add_subplot(2, 2, 1)\n",
    "    ax.plot(x_axis_train, results['train_elbo'])\n",
    "    ax.set_ylabel('Loss (ELBO)')\n",
    "    ax.set_title('Loss (ELBO)')\n",
    "    ax.legend(['Train'], loc='upper right')\n",
    "\n",
    "    # Accuracy\n",
    "    ax = fig.add_subplot(2, 2, 2)\n",
    "    ax.plot(x_axis_train, results['accuracy_train'])\n",
    "    ax.plot(x_axis_test, results['accuracy_test'])\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy')\n",
    "    ax.legend(['Train', 'Test'], loc='lower right')\n",
    "\n",
    "    # ROC\n",
    "    ax = fig.add_subplot(2, 2, 3)\n",
    "    ax.plot(x_axis_train, results['roc_train'])\n",
    "    ax.plot(x_axis_test, results['roc_test'])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('ROC AUC')\n",
    "    ax.set_title('ROC AUC')\n",
    "    ax.legend(['Train', 'Test'], loc='lower right')\n",
    "\n",
    "    # Precision\n",
    "    ax = fig.add_subplot(2, 2, 4)\n",
    "    ax.plot(x_axis_train, results['ap_train'])\n",
    "    ax.plot(x_axis_test, results['ap_test'])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision')\n",
    "    ax.legend(['Train', 'Test'], loc='lower right')\n",
    "\n",
    "    # Save\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using citeseer dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cce4ce27ddb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-fc5ac7f7f866>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "args = dotdict()\n",
    "args.seed        = 2\n",
    "args.dropout     = 0.0\n",
    "args.num_epochs  = 50\n",
    "# args.dataset_str = 'cora'\n",
    "args.dataset_str = 'citeseer'\n",
    "args.test_freq   = 10\n",
    "args.lr          = 0.01\n",
    "\n",
    "pyro.clear_param_store()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
