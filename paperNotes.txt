Deep Learning on Graphs: A Survey 

Semi-Supervised Methods
    - Graph Neural Networks
    - Graph CNNs

Unsupervised
    - Graph Autoencoders 

Recent Advancements
    - Graph RNN
    - Graph RL

Intro
    - Irregular domain (hard to generalize some basic math ops like conv and pooling, Geometric DL Problem)
    - Varying Structure and tasks (hetero/homo-geneous, weighted or unweighted, signed or unsigned, node vs. graph-focused problems)
    - Scalability and Parallelization (especailly with Linear time complexity)
    - Interdisciplinary Approaches 
    - Difference between these methods and network embedding
    - NODE (node classification, link prediction, node recommendation) VS. GRAPH FOCUSED TASKS (graph classification, estimation of graph properties, generating graphs)

Graph Neural Networks (GNNs)
    - Task: Encode structural information of the graph with a low-dimensional state vector
        - Use the recursive defintion of nn states, solve with Jacobi method
    - Limited because a unique solution must be found, computationally Expensive

Graph Convolutional Networks (GCNs)
    - Graph data from the spectral domain is represented using the LAPLACIAN MATRIX
        - USE TO CONSTRUCT LOW-DIMENSIONAL EMBEDDINGS OF A GRAPH
    - Similar geometric properties to CNNs
    - Learning params of convolution layer requires O(N) parameters, O(N^2) per forward and backward pass

Graph Autoencoders (GAEs)
    - Adjacency matrix (or its variations) regarded as raw features of nodes
    - Can be used as a dimensionality reduction technique to learne low-dimensional node representations
        - Uses the L2 reconstruction loss (nodes share similar latent representations if they have similar neighborhoods)
    
    Graph VAE:
        - DVNE proposes representing nodes in a graph as a Gaussian Distribution, uses Wasserstein distance

    - DRNE[79] proposes directly reconstructing low-d vectors of nodes by aggregating neighborhood info using LSTMs, new objective function
        - Order neighborhoods according to degrees 
    - Laplacian Matrix vs TRANSITION PROBABILTY MATRIX vs. ADJACENCY vs. ADJACENCY LISTS

Improvements:
    - Adversarial Training
        -  used with Graph Autoencoders as an additional regularization term 
            - Encoder is generator, discriminator aims to distinguish whether a latent distribution if from the generator or a prior '
    - different similarity measures

Recent Advancements
    - GraphRNNs 
        - Graph generation problem. Two RNNs, one for nodes, one for edges (in an autoregressive manner)
        - DGNN uses time-aware LSTM to learn node representations in dynamic graphs
    - GraphRNN with GraphAE
        - ADVERSARIALLY REGULARIZED GRAPH AUTOENCODERS WITH GRAPHRNN
            - LAPLACIAN EIGENMAP (for embedding), L2 LOSS, ranking loss, Kl, Wasserstein distance
            - TIME-AWARE LSTM