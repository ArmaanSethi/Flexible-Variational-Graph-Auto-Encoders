{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from IPython import embed\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd.variable as Variable\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric.utils as torch_util\n",
    "import torch_scatter\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MNISTSuperpixels, Planetoid\n",
    "from torch_geometric.nn import ChebConv, GCNConv, SAGEConv\n",
    "\n",
    "from pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "import networkx as nx\n",
    "import visdom\n",
    "import scipy as sp\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the PyTorch module that parameterizes the\n",
    "# diagonal gaussian distribution q(z|x)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_feat, hidden_dim, latent_dim, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Set up thhe Graph convolutional layers \n",
    "        self.gc1 = GCNConv(n_feat, hidden_dim)\n",
    "        self.gc2_mu = GCNConv(hidden_dim, latent_dim)\n",
    "        self.gc2_sig = GCNConv(hidden_dim, latent_dim)\n",
    "        \n",
    "        # self.gc1 = SAGEConv(n_feat, hidden_dim)\n",
    "        # self.gc2_mu = SAGEConv(hidden_dim, z_dim)\n",
    "        # self.gc2_sig = SAGEConv(hidden_dim, z_dim)    \n",
    "        \n",
    "        # Setup for non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # define the forward computation on the adjacency matrix for each graph and its features, x \n",
    "\n",
    "        print('Features: ', x, x.shape)\n",
    "        print(\"Adj: \", adj, adj.shape)\n",
    "        \n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        # Sharing parameters between mu and sigma \n",
    "        z_loc = self.gc2_mu(x, adj)\n",
    "        z_scale = torch.exp(self.gc2_sig(x, adj))\n",
    "\n",
    "#         hidden = self.softplus(self.gc1(x, adj))\n",
    "#         z_loc = self.gc2_mu(hidden, adj)\n",
    "#         z_scale = torch.exp(self.gc2_sig(hidden, adj))\n",
    "\n",
    "        return z_loc, z_scale\n",
    "\n",
    "\n",
    "# TODO: Modify from Inner Product decoder \n",
    "# define the PyTorch module that parameterizes the observation likelihood p(x|z)\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, z_dim, hidden_dim):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         # setup the two linear transformations used for the decoder \n",
    "#         self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "#         self.fc21 = nn.Linear(hidden_dim, 75)\n",
    "#         # setup the non-linearities\n",
    "#         self.softplus = nn.Softplus()\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         # define the forward computation on the latent z\n",
    "#         # first compute the hidden units\n",
    "\n",
    "#         hidden = self.softplus(self.fc1(z))\n",
    "\n",
    "#         # return the parameter for the output Bernoulli\n",
    "#         loc_img = torch.sigmoid(self.fc21(hidden))\n",
    "#         return loc_img\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.fudge = 1e-7\n",
    "    \n",
    "    def forward(self, z): \n",
    "        z = F.dropout(z, self.dropout, training=self.training)\n",
    "        adj = (nn.Sigmoid(torch.mm(z, z.t())) + self.fudge) * (1 - 2 * self.fudge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(nn.Module):\n",
    "    \"\"\"Graph Auto Encoder (see: https://arxiv.org/abs/1611.07308)\"\"\"\n",
    "\n",
    "    def __init__(self, data, n_hidden, n_latent, dropout, subsampling=False):\n",
    "        super(VGAE, self).__init__()\n",
    "\n",
    "        # Data\n",
    "        self.x = data['features']\n",
    "        self.adj_norm = data['adj_norm']\n",
    "        self.adj_labels = data['adj_labels']\n",
    "        self.obs = self.adj_labels.view(1, -1)\n",
    "\n",
    "        # Dimensions\n",
    "        N, D = data['features'].shape\n",
    "        self.n_samples = N\n",
    "        self.n_edges = self.adj_labels.sum()\n",
    "        self.n_subsample = 2 * self.n_edges\n",
    "        self.input_dim = D\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_latent = n_latent\n",
    "\n",
    "        # Parameters\n",
    "        self.pos_weight = float(N * N - self.n_edges) / self.n_edges\n",
    "        self.norm = float(N * N) / ((N * N - self.n_edges) * 2)\n",
    "        self.subsampling = subsampling\n",
    "\n",
    "        # Layers\n",
    "        self.dropout = dropout\n",
    "        self.encoder = Encoder(self.input_dim, self.n_hidden, self.n_latent, self.dropout)\n",
    "        self.decoder = Decoder(self.dropout)\n",
    "\n",
    "\n",
    "    def model(self):\n",
    "        # register PyTorch module `decoder` with Pyro\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "\n",
    "        # Setup hyperparameters for prior p(z)\n",
    "        z_mu    = torch.zeros([self.n_samples, self.n_latent])\n",
    "        z_sigma = torch.ones([self.n_samples, self.n_latent])\n",
    "\n",
    "        # sample from prior\n",
    "        z = pyro.sample(\"latent\", dist.Normal(z_mu, z_sigma).to_event(2))\n",
    "\n",
    "        # decode the latent code z\n",
    "        z_adj = self.decoder(z).view(1, -1)\n",
    "\n",
    "        # Score against data\n",
    "        pyro.sample('obs', WeightedBernoulli(z_adj, weight=self.pos_weight).to_event(2), obs=self.obs)\n",
    "\n",
    "\n",
    "    def guide(self):\n",
    "        # register PyTorch model 'encoder' w/ pyro\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "\n",
    "        # Use the encoder to get the parameters use to define q(z|x)\n",
    "        print(self.x.shape)\n",
    "        print(self.adj_norm.shape)\n",
    "        print(self.x)\n",
    "        print(self.adj_norm)\n",
    "                \n",
    "        z_mu, z_sigma = self.encoder(self.x, self.adj_norm)\n",
    "\n",
    "        # Sample the latent code z\n",
    "        pyro.sample(\"latent\", dist.Normal(z_mu, z_sigma).to_event(2))\n",
    "\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        z_mu, _ = self.encoder.eval()(self.x, self.adj_norm)\n",
    "        # Put encoder back into training mode\n",
    "        self.encoder.train()\n",
    "        return z_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Get Cora data\n",
    "def get_data():\n",
    "    dataset = args.name\n",
    "    \n",
    "    if dataset == 'MNIST':\n",
    "        path = '../data/geometric/MNIST'\n",
    "        trainset = MNISTSuperpixels(path, train=True)\n",
    "        testset = MNISTSuperpixels(path, train=False)\n",
    "\n",
    "        lenTrain = len(trainset)\n",
    "        lenTest = len(testset)\n",
    "\n",
    "        trainLoader = DataLoader(trainset[:lenTrain//125], batch_size=1, shuffle=False)\n",
    "        testloader = DataLoader(testset[:lenTest//125], batch_size=1, shuffle=False)\n",
    "        return trainLoader, testloader\n",
    "\n",
    "    elif dataset == 'CORA':\n",
    "        dataset = 'Cora'\n",
    "        path = path = '../data/geometric/CORA'\n",
    "        dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "\n",
    "        return dataset, _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CORA dataset\n",
      "preprocess finished\n"
     ]
    }
   ],
   "source": [
    "# Load in Data \n",
    "print('Using {} dataset'.format(args.name))\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "train_loader, test_loader = get_data()\n",
    "data = train_loader[0]\n",
    "\n",
    "# Get features and store their shape\n",
    "features = np.array(data['x'])\n",
    "N, D = features.shape\n",
    "\n",
    "edgeList = np.array(data['edge_index'].transpose(1,0 ))\n",
    "adj = nx.adjacency_matrix(nx.from_edgelist(edgeList))    \n",
    "\n",
    "adj_original = adj\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges = mask_edges(adj)\n",
    "\n",
    "adj_train_norm   = preprocess_graph(adj_train)\n",
    "adj_train_norm   = Variable(make_sparse(adj_train_norm))\n",
    "adj_train_labels = Variable(torch.FloatTensor(adj_train + sp.sparse.eye(adj_train.shape[0]).todense()))\n",
    "\n",
    "features = Variable(make_sparse(features))\n",
    "n_edges = adj_train_labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49216, 1])\n",
      "torch.Size([2, 11684])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# feat = data['features']\n",
    "# adj = data['adj_norm']\n",
    "\n",
    "# feat._values().resize_((feat._values().shape[0], 1))\n",
    "\n",
    "\n",
    "# # print(feat._values())\n",
    "# # print(adj._indices().shape)\n",
    "\n",
    "# featOut = feat._values()\n",
    "# adjOut = adj._indices()\n",
    "\n",
    "# print(featOut, featOut.shape)\n",
    "# print(adjOut, adjOut.shape)\n",
    "\n",
    "feat = features._values().resize_((features._values().shape[0], 1))\n",
    "print(feat.shape)\n",
    "print(adj_train_norm._indices().shape)\n",
    "\n",
    "# data = {\n",
    "#     'adj_norm'  : adj_train_norm,\n",
    "#     'adj_labels': adj_train_labels,\n",
    "#     'features'  : features,\n",
    "# }\n",
    "\n",
    "data = {\n",
    "    'adj_norm'  : adj_train_norm._indices(),\n",
    "    'adj_labels': adj_train_labels,\n",
    "    'features'  : features._values().resize_((features._values().shape[0], 1)),\n",
    "}\n",
    "\n",
    "# print(data['adj_norm'])\n",
    "# print(data['features'])\n",
    "\n",
    "vgae = VGAE(data, \n",
    "            n_hidden=32,\n",
    "            n_latent=16,\n",
    "            dropout=args.dropout\n",
    "           )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model, data = VGAE().to(device), data.to(device)\n",
    "    \n",
    "optimizer = Adam({\"lr\": args.lr, \"betas\": (0.95, 0.999)})\n",
    "\n",
    "\n",
    "svi = SVI(vgae.model, vgae.guide, optimizer, loss=Trace_ELBO())\n",
    "results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49216, 1])\n",
      "torch.Size([2, 11684])\n",
      "tensor([[0.1111],\n",
      "        [0.1111],\n",
      "        [0.1111],\n",
      "        ...,\n",
      "        [0.0769],\n",
      "        [0.0769],\n",
      "        [0.0769]])\n",
      "tensor([[   0,    0,    0,  ..., 2706, 2707, 2707],\n",
      "        [   3,    2,    1,  ..., 2706, 2707,  840]])\n",
      "Features:  tensor([[0.1111],\n",
      "        [0.1111],\n",
      "        [0.1111],\n",
      "        ...,\n",
      "        [0.0769],\n",
      "        [0.0769],\n",
      "        [0.0769]]) torch.Size([49216, 1])\n",
      "Adj:  tensor([[   0,    0,    0,  ..., 2706, 2707, 2707],\n",
      "        [   3,    2,    1,  ..., 2706, 2707,  840]]) torch.Size([2, 11684])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-471-a67ae46f0fb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# do ELBO gradient and accumulate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msvi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# report training diagnostics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lab\\lib\\site-packages\\pyro\\infer\\svi.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# get loss and compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lab\\lib\\site-packages\\pyro\\infer\\trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;31m# grab a trace from the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mguide_trace\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m             \u001b[0mloss_particle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_differentiable_loss_particle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_particle\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lab\\lib\\site-packages\\pyro\\infer\\elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\lab\\lib\\site-packages\\pyro\\infer\\trace_elbo.py\u001b[0m in \u001b[0;36m_get_trace\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \"\"\"\n\u001b[0;32m     51\u001b[0m         model_trace, guide_trace = get_importance_trace(\n\u001b[1;32m---> 52\u001b[1;33m             \"flat\", self.max_plate_nesting, model, guide, *args, **kwargs)\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mcheck_if_enumerated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mguide_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lab\\lib\\site-packages\\pyro\\infer\\enum.py\u001b[0m in \u001b[0;36mget_importance_trace\u001b[1;34m(graph_type, max_plate_nesting, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mguide_trace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mguide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     model_trace = poutine.trace(poutine.replay(model, trace=guide_trace),\n\u001b[1;32m---> 44\u001b[1;33m                                 graph_type=graph_type).get_trace(*args, **kwargs)\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mcheck_model_guide_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_plate_nesting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lab\\lib\\site-packages\\pyro\\poutine\\trace_messenger.py\u001b[0m in \u001b[0;36mget_trace\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mCalls\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mpoutine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mits\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \"\"\"\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmsngr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lab\\lib\\site-packages\\pyro\\poutine\\trace_messenger.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m                                       args=args, kwargs=kwargs)\n\u001b[0;32m    146\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m                 \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lab\\lib\\site-packages\\pyro\\poutine\\messenger.py\u001b[0m in \u001b[0;36m_wraps\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_wraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0m_wraps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmsngr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wraps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-467-a1f14639cec3>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# decode the latent code z\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mz_adj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Score against data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-466-e2ee9bcf6c77>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0madj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfudge\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfudge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do ELBO gradient and accumulate loss\n",
    "    epoch_loss += svi.step()\n",
    "\n",
    "    # report training diagnostics\n",
    "    normalized_loss = epoch_loss / (2 * N * N)\n",
    "\n",
    "    results['train_elbo'].append(normalized_loss)\n",
    "\n",
    "    # Training loss\n",
    "    emb = gae.get_embeddings()\n",
    "\n",
    "    accuracy, roc_curr, ap_curr = eval_gae(val_edges, val_edges_false, emb, adj_orig)\n",
    "\n",
    "    results['accuracy_train'].append(accuracy)\n",
    "    results['roc_train'].append(roc_curr)\n",
    "    results['ap_train'].append(ap_curr)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "          \"train_loss=\", \"{:.5f}\".format(normalized_loss),\n",
    "          \"train_acc=\", \"{:.5f}\".format(accuracy), \"val_roc=\", \"{:.5f}\".format(roc_curr), \"val_ap=\", \"{:.5f}\".format(ap_curr))\n",
    "\n",
    "    # Test loss\n",
    "    if epoch % args.test_freq == 0:\n",
    "        emb = gae.get_embeddings()\n",
    "        accuracy, roc_score, ap_score = eval_gae(test_edges, test_edges_false, emb, adj_orig)\n",
    "        results['accuracy_test'].append(accuracy)\n",
    "        results['roc_test'].append(roc_curr)\n",
    "        results['ap_test'].append(ap_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pyro.__version__.startswith('0.3.1')\n",
    "# parse command line arguments\n",
    "parser = argparse.ArgumentParser(description=\"parse args\")\n",
    "parser.add_argument('--num-epochs', default=2, type=int, help='number of training epochs')\n",
    "parser.add_argument('--tf', '--test-frequency', default=5, type=int, help='how often we evaluate the test set')\n",
    "parser.add_argument('--lr', '--learning-rate', default=1.0e-3, type=float, help='learning rate')\n",
    "    \n",
    "parser.add_argument('--cuda', action='store_true', default=False, help='whether to use cuda')\n",
    "parser.add_argument('--jit', action='store_true', default=False, help='whether to use PyTorch jit')\n",
    "parser.add_argument('-visdom', '--visdom_flag', action=\"store_true\", help='Whether plotting in visdom is desired')\n",
    "parser.add_argument('--time', default=int(time.time()), help=\"Current system time\")\n",
    "\n",
    "parser.add_argument('--name', default='CORA', help=\"Name of the dataset\")\n",
    "parser.add_argument('--save', default=False, help=\"Whether to save the trained model\")\n",
    "parser.add_argument('--dropout', default=0, help=\"Dropout probability\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # adj2 = nx.adjacency_matrix(nx.from_edgelist(np.array(adj._indices().transpose(1, 0))))\n",
    "# # print(torch.sparse.FloatTensor(adj2.tocoo()))\n",
    "# # edgeList = np.array(data['edge_index'].transpose(1,0 ))\n",
    "# gc = GCNConv(D, 1)\n",
    "\n",
    "# out = gc(featOut, adjOut)\n",
    "\n",
    "# # torch.Size([75, 1])\n",
    "# # Adj     :  tensor([[ 0,  0,  0,  ..., 74, 74, 74],\n",
    "# #         [ 3,  8, 10,  ..., 55, 63, 69]]) torch.Size([2, 1399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_graph(adj):\n",
    "#     adj = sp.sparse.coo_matrix(adj)\n",
    "#     adj_ = adj + sp.eye(adj.shape[0])\n",
    "#     rowsum = np.array(adj_.sum(1))\n",
    "#     degree_mat_inv_sqrt = sp.sparse.diags(np.power(rowsum, -0.5).flatten())\n",
    "#     adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(\n",
    "#         degree_mat_inv_sqrt).tocoo()\n",
    "#     return adj_normalized\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.sparse.coo_matrix(adj)\n",
    "    adj_ = adj + sp.sparse.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.sparse.diags(np.power(rowsum, -.5).flatten())\n",
    "    coodot = adj_.dot(degree_mat_inv_sqrt).tocoo()\n",
    "    adj_normalized = coodot.transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    print('preprocess finished')\n",
    "    return adj_normalized\n",
    "    \n",
    "\n",
    "def make_sparse(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "#     sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    sparse_mx = sp.sparse.coo_matrix(sparse_mx, dtype=np.float32)\n",
    "    indices = torch.Tensor(np.array(np.vstack((sparse_mx.row, sparse_mx.col)), dtype=np.float32)).long()\n",
    "\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.sparse.FloatTensor(torch.from_numpy(sparse_mx.data))\n",
    "#     values = torch.from_numpy(sparse_mx.data)\n",
    "\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "#     return torch.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.sparse.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "def ismember(a, b, tol=5):\n",
    "    rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "    return np.any(rows_close)\n",
    "\n",
    "def mask_edges(adj):\n",
    "    # Function to build test set with 10% positive links\n",
    "    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
    "    # TODO: Clean up.\n",
    "\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - \\\n",
    "        sp.sparse.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    adj_triu = sp.sparse.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    \n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack(\n",
    "        [test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    assert ~ismember(test_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges, train_edges)\n",
    "    assert ~ismember(test_edges, train_edges)\n",
    "    assert ~ismember(val_edges, test_edges)\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.sparse.csr_matrix(\n",
    "        (data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    # NOTE: these edge lists only contain single direction of edge!\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "def eval_gae(edges_pos, edges_neg, emb, adj_orig):\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    emb = emb.data.numpy()\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "\n",
    "    accuracy = accuracy_score((preds_all > 0.5).astype(float), labels_all)\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return accuracy, roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
